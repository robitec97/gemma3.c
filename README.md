# gemma3.c

`gemma3.c` is a **fromâ€‘scratch CPU inference engine** for the *Gemma 3 4B IT* model.

## âœ¨ Highlights

* âš™ï¸ **100% Pure C (C11)** â€“ zero external dependencies
* ğŸ§  **Full Gemma 3 architecture** â€“ GQA, hybrid attention, SwiGLU
* ğŸ—ºï¸ **Memoryâ€‘mapped weights** â€“ BF16 SafeTensors via `mmap`
* ğŸ”¤ **Native SentencePiece tokenizer** â€“ 262K vocab
* ğŸŒŠ **Streaming output** â€“ tokenâ€‘byâ€‘token callbacks
* ğŸ’¬ **Interactive chat mode**
* ğŸ“¦ **CLI + Library API**
* ğŸ§ **Linux/macOS native**, ğŸªŸ Windows via **WSL** (recommended) or **MinGW**
* ğŸ”— **OpenBLAS support** (optional) â€“ BLAS-accelerated matrix operations
* ğŸ§µ **Multi-threaded inference** â€“ Thread pool for parallel computation

---

## ğŸš€ Quick Start

> âš ï¸ POSIXâ€‘first: native on Linux/macOS. On Windows use **WSL** or **MinGW** (no `mmap`).

### 1ï¸âƒ£ Download model (recommended)

```bash
export HF_TOKEN=your_token_here
python download_model.py
```

### 2ï¸âƒ£ Build

```bash
make
```

### 3ï¸âƒ£ Run

```bash
# Single prompt
./gemma3 -m ./gemma-3-4b-it -p "Explain quantum computing simply."

# Interactive chat
./gemma3 -m ./gemma-3-4b-it -i
```

> **OpenBLAS builds:** `make blas` and `make blas-threads` require OpenBLAS:
> - Linux: `sudo apt install libopenblas-dev`
> - macOS: `brew install openblas`

---

## ğŸ“¥ Model Download

The included Python script:

* Handles HuggingFace auth
* Downloads all shards
* Resumes broken downloads
* Verifies integrity

```bash
python download_model.py --token YOUR_HF_TOKEN
```

Manual alternatives: `huggingface-cli` or `git lfs`.

---

## ğŸ› ï¸ Build Targets

```bash
make              # Release build (default)
make debug        # Debug symbols
make fast         # Native optimizations (-march=native -ffast-math)
make threads      # Thread pool parallelization
make blas         # OpenBLAS acceleration (requires libopenblas)
make blas-threads # OpenBLAS + threads (best performance)
make clean        # Remove build artifacts
make help         # Show all targets
```

---

## ğŸ§ª CLI Options

```
-m <path>    Model directory
-p <text>    Prompt
-i           Interactive mode
-s <text>    System prompt
-n <n>       Max tokens
-t <f>       Temperature
-k <n>       Topâ€‘k
--top-p <f>  Topâ€‘p
-c <n>       Context size
--seed <n>   RNG seed
-v           Verbose
```

---

## ğŸ“š Library Example

```c
gemma3_ctx *ctx = gemma3_load_dir("./gemma-3-4b-it");

gemma3_gen_params params = gemma3_default_params();
char *out = gemma3_generate(ctx, "Hello!", &params, NULL, NULL);
printf("%s\n", out);
free(out);

gemma3_free(ctx);
```

---

## ğŸ§  Model Specs

| Param   | Value              |
| ------- | ------------------ |
| Vocab   | 262,208            |
| Layers  | 34                 |
| Hidden  | 2,560              |
| Heads   | 8 (4 KV, GQA)      |
| Context | 128K               |
| Pattern | 5 local : 1 global |

---

## ğŸ’¾ Memory

* Weights: ~8â€¯GB on disk (BF16)
* Runtime RAM: **~3â€¯GB total**

Reduce usage:

```bash
./gemma3 -m ./gemma-3-4b-it -c 512 -p "Hello"
```

---

## âš¡ Performance (CPU)

* Prefill: ~2â€“5 tok/s
* Generation: ~1â€“3 tok/s

For better performance:

```bash
make fast          # Single-threaded with native optimizations
make threads       # Multi-core parallelization
make blas-threads  # Best performance (requires OpenBLAS)
```

---

## âš ï¸ Limitations

* CPU only
* Text only
* No quantization (yet)

---

## ğŸªª License

MIT License.
Model weights under Googleâ€™s Gemma license.

---

*If you ever wanted to see Gemma 3 breathe in pure C, this is it.*
